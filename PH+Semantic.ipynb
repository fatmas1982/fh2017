{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_path = '/home/cloudera/cs.txt'\n",
    "def read_file(str):\n",
    "    file = open(str,'r')\n",
    "    txt=file.read()\n",
    "    #print(txt)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "txt=read_file(new_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Split text to pragraphs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def txt_pragraphs(str):\n",
    "    pragraphs = str.split(\"\\n\\n\")\n",
    "    return pragraphs\n",
    "pragraphs=txt_pragraphs(txt)\n",
    "type(pragraphs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComputer science is a discipline that spans theory and practice. It requires thinking both in abstract terms and in concrete terms. The practical side of computing can be seen everywhere. Nowadays, practically everyone is a computer user, and many people are even computer programmers.\\nGetting computers to do what you want them to do requires intensive hands-on experience. But computer science can be seen on a higher level, as a science of problem solving. Computer scientists must be adept at modeling and analyzing problems. They must also be able to design solutions and verify that they are correct. Problem solving requires precision, creativity, and careful reasoning.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pragraphs[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Split Pragraph to Sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#print(sent_tokenize(pragraphs(txt)[1]))\n",
    "def pragraph_to_setnences(str):\n",
    "    return sent_tokenize(str)\n",
    "setnences=pragraph_to_setnences(pragraphs[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word Process For Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Removing English stopwords and Punct per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
    "new_stop_words2=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "from stemming.porter2 import stem\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "#stemmer = PorterStemmer()\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "def remove_stopword_sentences(str):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    words=tokenizer.tokenize(str)\n",
    "    english_stops = set(stopwords.words('english'))\n",
    "    stems=[]\n",
    "    list_word=[word for word in words if word.lower() not in english_stops and word.lower() not in new_stop_words and word.lower() not in new_stop_words2]\n",
    "    for word in list_word:\n",
    "        #stems.append(stem(word))\n",
    "        #stems.append(PorterStemmer().stem(word))\n",
    "        #stems.append(stemmer.stem(word))\n",
    "        #stems.append(stemmer.stem(\"computer\"))\n",
    "        stems.append(word)\n",
    "    \n",
    "    return stems#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nowadays', 'practically', 'everyone', 'computer', 'user', 'many', 'people', 'even', 'computer', 'programmers']\n",
      "Counter({'computer': 2, 'even': 1, 'many': 1, 'user': 1, 'Nowadays': 1, 'people': 1, 'programmers': 1, 'everyone': 1, 'practically': 1})\n"
     ]
    }
   ],
   "source": [
    "words=remove_stopword_sentences(setnences[3])\n",
    "count = Counter(words)\n",
    "# to count words\n",
    "#print (count.most_common(10))\n",
    "print(words)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>even</th>\n",
       "      <th>many</th>\n",
       "      <th>user</th>\n",
       "      <th>Nowadays</th>\n",
       "      <th>people</th>\n",
       "      <th>computer</th>\n",
       "      <th>programmers</th>\n",
       "      <th>everyone</th>\n",
       "      <th>practically</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      even  many  user  Nowadays  people  computer  programmers  everyone  \\\n",
       "Freq     1     1     1         1       1         2            1         1   \n",
       "\n",
       "      practically  \n",
       "Freq            1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def compute_freq(word_vec):\n",
    "    headers=[\"Freq\"]\n",
    "    \n",
    "    word_nonfreq={}\n",
    "    \n",
    "    for index in range(len(word_vec)):\n",
    "        #word_nonfreq.append(word)\n",
    "        \n",
    "        for index2 in range(len(word_vec)):\n",
    "            counter=0\n",
    "            if index2+1 in range(len(word_vec)):\n",
    "                print (word_vec[index],word_vec[index2+1])\n",
    "                if word_vec[index]==word_vec[index2+1]:\n",
    "                    counter+=1\n",
    "        word_nonfreq[ word_vec[index]]=counter\n",
    "        print(counter)\n",
    "        #df_frequencies.loc[word]=counter\n",
    "    df_frequencies = pd.DataFrame(word_nonfreq,index=headers, columns=word_nonfreq ) \n",
    "    \n",
    "    \n",
    "    return df_frequencies\n",
    "\n",
    "\n",
    "def count_freq(word_vec):\n",
    "    headers=[\"Freq\"]\n",
    "    count = Counter(word_vec)\n",
    "    df_frequencies = pd.DataFrame(count,index=headers, columns=count ) \n",
    "    #print(count)\n",
    "    \n",
    "    return df_frequencies\n",
    "            \n",
    "#ss=compute_freq(words)\n",
    "xx=count_freq(words)\n",
    "xx\n",
    "#list(xx)\n",
    "#ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Word Sense Disambiguation (WSD): LESK per Sentence\n",
    "Given an ambiguous word and the context in which the word occurs, Lesk returns a Synset with the highest number of overlapping words between the context sentence and different definitions from each Synset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "'''\n",
    "this function for compute lesk for each word(list of word) in sentence\n",
    "'''\n",
    "def lesk_words_sentence(words,sentence):\n",
    "    lesks= []\n",
    "    for word in words:\n",
    "        if lesk(sentence,word, 'n') is not None:\n",
    "            lesks.append(lesk(sentence,word, 'n'))\n",
    "            #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "this function for compute lesk of word in sentence\n",
    "'''\n",
    "\n",
    "def lesk_word_sentence(word,sentence):\n",
    "    #lesks= []\n",
    "    #for word in words:\n",
    "    lesk_synset=lesk(sentence,word, 'n')\n",
    "        #print(\"Word is: \",word,\"\\n LESK: \",lesk(sentence,word, 'n'),\"\\n Sentence: \",sentence )\n",
    "        \n",
    "    return lesk_synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lesks=lesk_words_sentence(words,setnences[0])\n",
    "leskss=lesk_word_sentence(words[1],setnences[0])\n",
    "#print(words[1],setnences[0])\n",
    "#print (leskss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#type(lesks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Calculating WordNet Synset similarity using information content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic as wnic\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "'''\n",
    "this function for compute similarty between 2 synset \n",
    "'''\n",
    "\n",
    "def similarity_by_infocontent(sense1, sense2, option):\n",
    "    \"\"\" Returns similarity scores by information content. \"\"\"\n",
    "    #if sense1.pos != sense2.pos: # infocontent sim can't do diff POS.\n",
    "        #return 0\n",
    "\n",
    "    info_contents = ['ic-bnc-add1.dat', 'ic-bnc-resnik-add1.dat', \n",
    "                     'ic-bnc-resnik.dat', 'ic-bnc.dat', \n",
    "\n",
    "                     'ic-brown-add1.dat', 'ic-brown-resnik-add1.dat', \n",
    "                     'ic-brown-resnik.dat', 'ic-brown.dat', \n",
    "\n",
    "                     'ic-semcor-add1.dat', 'ic-semcor.dat',\n",
    "\n",
    "                     'ic-semcorraw-add1.dat', 'ic-semcorraw-resnik-add1.dat', \n",
    "                     'ic-semcorraw-resnik.dat', 'ic-semcorraw.dat', \n",
    "\n",
    "                     'ic-shaks-add1.dat', 'ic-shaks-resnik.dat', \n",
    "                     'ic-shaks-resnink-add1.dat', 'ic-shaks.dat', \n",
    "\n",
    "                     'ic-treebank-add1.dat', 'ic-treebank-resnik-add1.dat', \n",
    "                     'ic-treebank-resnik.dat', 'ic-treebank.dat']\n",
    "\n",
    "    if option in ['res', 'resnik']:\n",
    "        #return wn.res_similarity(sense1, sense2, wnic.ic('ic-bnc-resnik-add1.dat'))\n",
    "        return wn.res_similarity(sense1, sense2, wnic.ic('ic-treebank-resnik-add1.dat'))\n",
    "    #return min(wn.res_similarity(sense1, sense2, wnic.ic(ic)) \\\n",
    "    #             for ic in info_contents)\n",
    "\n",
    "    elif option in ['jcn', \"jiang-conrath\"]:\n",
    "        #return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "    elif option in ['lin']:\n",
    "        #return wn.lin_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
    "        return wn.lin_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
    "\n",
    "def sim(sense1, sense2, option=\"path\"):\n",
    "    \"\"\" Calculates similarity based on user's choice. \"\"\"\n",
    "    option = option.lower()\n",
    "    if option.lower() in [\"path\", \"path_similarity\", \n",
    "                        \"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\",\n",
    "                        'lch', \"leacock-chordorow\"]:\n",
    "        return similarity_by_path(sense1, sense2, option) \n",
    "    elif option.lower() in [\"res\", \"resnik\",\n",
    "                          \"jcn\",\"jiang-conrath\",\n",
    "                          \"lin\"]:\n",
    "        return similarity_by_infocontent(sense1, sense2, option)\n",
    "\n",
    "def max_similarity(context_sentence, ambiguous_word, option=\"path\", \n",
    "                   pos=None, best=True):\n",
    "    \"\"\"\n",
    "    Perform WSD by maximizing the sum of maximum similarity between possible \n",
    "    synsets of all words in the context sentence and the possible synsets of the \n",
    "    ambiguous words (see http://goo.gl/XMq2BI):\n",
    "    {argmax}_{synset(a)}(\\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for i in wn.synsets(ambiguous_word):\n",
    "        try:\n",
    "            if pos and pos != str(i.pos()):\n",
    "                continue\n",
    "        except:\n",
    "            if pos and pos != str(i.pos):\n",
    "                continue\n",
    "        result[i] = sum(max([sim(i,k,option) for k in wn.synsets(j)]+[0]) \\\n",
    "                        for j in word_tokenize(context_sentence))\n",
    "\n",
    "    if option in [\"res\",\"resnik\"]: # lower score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()])\n",
    "    else: # higher score = more similar\n",
    "        result = sorted([(v,k) for k,v in result.items()],reverse=True)\n",
    "    #print (result)\n",
    "    if best: return result[0][1];\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('user.n.01') Synset('computer.n.01')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.5520118749329381"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lesks[2],lesks[1])\n",
    "similarity_by_infocontent(lesks[2],lesks[1],'res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#wn.res_similarity(lesks[1],lesks[2], wnic.ic('ic-bnc-resnik-add1.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#similarity_by_infocontent(lesks[2],lesks[1],'jcn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#similarity_by_infocontent(lesks[2],lesks[1],'lin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Simlarty per Words for Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df_sentence_similarty = pd.DataFrame(index=words, columns=words )\n",
    "#df_sentence_similarty.loc[words[0]]=[1,2,3,4,5,6]\n",
    "'''\n",
    "to retrive similarty between list of synset\n",
    "'''\n",
    "rows_list = []\n",
    "def simlarty_perWords(list_words,option):\n",
    "    for index in range(len(list_words)):\n",
    "        if index+1 <len(list_words):\n",
    "            #df_similarty.loc[list_words[index]]\n",
    "            #print(list_words[index].lowest_common_hypernyms(list_words[index+1]))\n",
    "            print(list_words[index],list_words[index+1],similarity_by_infocontent(list_words[index],list_words[index+1],option))\n",
    "            rows_list.append(similarity_by_infocontent(list_words[index],list_words[index+1],option))\n",
    "    return rows_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('present.n.01') Synset('computer.n.01') -0.0\n",
      "Synset('computer.n.01') Synset('user.n.01') 1.5520118749329381\n",
      "Synset('user.n.01') Synset('people.n.03') -0.0\n",
      "Synset('people.n.03') Synset('evening.n.01') 0.622354726626253\n",
      "Synset('evening.n.01') Synset('computer.n.01') -0.0\n",
      "Synset('computer.n.01') Synset('programmer.n.01') 1.5520118749329381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.0, 1.5520118749329381, -0.0, 0.622354726626253, -0.0, 1.5520118749329381]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simlarty_perWords(lesks,'res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#simlarty_perWords(lesks,'lin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#simlarty_perWords(lesks,'jcn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TF-IDF\n",
    "TF-IDF stands for \"Term Frequency, Inverse Document Frequency\". It is a way to score the importance of words (or \"terms\") in a document based on how frequently they appear across multiple documents.\n",
    "\n",
    "Intuitively...\n",
    "If a word appears frequently in a document, it's important. Give the word a high score.\n",
    "But if a word appears in many documents, it's not a unique identifier. Give the word a low score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tfidf\n",
    "#https://github.com/hrs/python-tf-idf\n",
    "table = tfidf.tfidf()\n",
    "table.addDocument(\"foo\", [lesks[0],lesks[1],lesks[3]])\n",
    "#table.addDocument(\"bar\", [\"alpha\", \"bravo\", \"charlie\", \"india\", \"juliet\", \"kilo\"])\n",
    "#table.addDocument(\"baz\", [\"kilo\", \"lima\", \"mike\", \"november\"])\n",
    "\n",
    "#print (table.similarities ([lesks[0],lesks[1],lesks[3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Pragraph Words Similarty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Compute similartyy per pragraph\n",
    "'''\n",
    "\n",
    "\n",
    "def similarty_pragraph(pragraph,option):\n",
    "    lesk_vec=[]# synset lesk of words per pragraph\n",
    "    synset_lesk_noDuplicates=[]\n",
    "    Sentences=pragraph_to_setnences(pragraph)\n",
    "    for sentence in Sentences:\n",
    "        Words=remove_stopword_sentences(sentence)\n",
    "        for word in Words:\n",
    "            lesk_synset=lesk_word_sentence(word,sentence)\n",
    "            if lesk_synset is not None:\n",
    "                lesk_vec.append(lesk_synset)\n",
    "\n",
    "    #Count Freq\n",
    "    df_freq=count_freq(lesk_vec)\n",
    "    #Removing Duplicates\n",
    "    series=list(df_freq)#pd.Series(data=lesk_vec)\n",
    "    #series.drop_duplicates()\n",
    "    synset_lesk_noDuplicates= series#.tolist()    \n",
    "    df_all_synset_lesk = pd.DataFrame(index=series, columns=series )            \n",
    "    print (\"synset_lesk_noDuplicates\",len(synset_lesk_noDuplicates))\n",
    "    \n",
    "\n",
    "\n",
    "    for row in range(len(synset_lesk_noDuplicates)):\n",
    "        try:\n",
    "            data_row=[]\n",
    "            for col in range(len(synset_lesk_noDuplicates)):\n",
    "\n",
    "                    #if row < col:\n",
    "\n",
    "                    sim=similarity_by_infocontent(series[row],series[col],option)\n",
    "                    if sim is not None:\n",
    "                        data_row.append(sim)\n",
    "                    else:\n",
    "                        data_row.append(0)\n",
    "                        #print(len(data_row))\n",
    "\n",
    "\n",
    "                    #else:\n",
    "\n",
    "                        #data_row.append(\"row>col\")\n",
    "                        #print(len(data_row))\n",
    "\n",
    "            print(series[row])\n",
    "            df_all_synset_lesk.loc[series[row]]=data_row\n",
    "        except  Exception as inst:\n",
    "                data_row.append(0)\n",
    "                print(type(inst))    # the exception instance\n",
    "                print(inst.args)     # arguments stored in .args\n",
    "                print(inst)          # __str__ allows args to be printed directly,\n",
    "                pass\n",
    "                #print(\"Ex\")\n",
    "    return df_all_synset_lesk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "res_df=similarty_pragraph(pragraphs[1],'res')\n",
    "res_df\n",
    "#jcn_df.to_csv('df_all_synset_lesk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "jcn_df=similarty_pragraph(pragraphs[1],'jcn')\n",
    "jcn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lin_df=similarty_pragraph(pragraphs[0],'lin')\n",
    "lin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot') \n",
    "import numpy as np\n",
    "#import bokeh.charts as bc\n",
    "#import bokeh.plotting as bk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "jcn_df.plot(title='jcn_df')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.random.rand(50, 4), columns=['a', 'b', 'c', 'd'])\n",
    "\n",
    "jcn_df.plot.scatter(x='a', y='b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ax = df.plot.scatter(x='a', y='b', color='DarkBlue', label='Group 1');\n",
    "df.plot.scatter(x='c', y='d', color='DarkGreen', label='Group 2', ax=ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "jcn_df.plot.scatter(x='a', y='b', c='c', s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "jcn_df.plot(style=['o','rx','kk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fetch_20newsgroups2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "v8OKXX82Gw1v",
        "H00aZ9MQIZoB",
        "G8DAsqXcJ3mT",
        "VAgZQlckK9wd",
        "_6BmvqLFLazK",
        "Iu4dgfNRLfkw",
        "fTKOLYu3L0Ez",
        "8NfaNVAwMFMu",
        "OQuKzaqfMkq5",
        "nocJy3-gN7hz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/fatmas1982/fh2017/blob/master/fetch_20newsgroups2_9.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "m_coVyipHSgW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Load All API"
      ]
    },
    {
      "metadata": {
        "id": "me14sBCEuffI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "addff492-de11-43ea-be6b-8a79456f4b50"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.style.use('ggplot') \n",
        "import numpy as np\n",
        "import scipy.stats.stats as st\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.wsd import lesk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "#from stemming.porter2 import stem\n",
        "from nltk import PorterStemmer\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from string import digits\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import csv\n",
        "\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "!pip install gensim\n",
        "import  gensim.models as md\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import os\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import wordnet_ic as wnic\n",
        "from nltk.tokenize import word_tokenize\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.5.7)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.48.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.7.16)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
            "Requirement already satisfied: botocore<1.11.0,>=1.10.16 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.10.16)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.4.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.16->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.16->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
            "[nltk_data] Downloading package punkt to /content/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /content/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v8OKXX82Gw1v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Authosentecat to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "GvYYCnMyytqW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install pydrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aikWxQjx05u4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fzwzSluP1BRq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SR-ozY_l1OaA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://colab.research.google.com/notebook#fileId=1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q&scrollTo=c99EvWo1s9-x\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M2eyeUwBmzyr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7VU7uSU82Uwy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GNP8Uhv525hA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive_PH\n",
        "!google-drive-ocamlfuse drive_PH"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YqDmfjblsCgs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls 'drive_PH/Colab Notebooks'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ODNbV0LnG_Za",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Start My Code"
      ]
    },
    {
      "metadata": {
        "id": "9EdkTMMcI70y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Settings **Codes**"
      ]
    },
    {
      "metadata": {
        "id": "rVChg9cLwyD6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_cvs_by_pands(path_database,file_databbase,index_col, header):\n",
        "    return pd.read_csv(path_database+file_databbase,index_col=index_col,header=header)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XVlUCEHeRxxC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def txt_pragraphs(str):\n",
        "    pragraphs = str.split(\"\\n\\n\")\n",
        "    return pragraphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QrY0paf2w7LA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_text_from_database(path_database,file_databbase):\n",
        "    queue_paragraph=[]\n",
        "    #f = open(sys.argv[1], 'rt')\n",
        "    outfile = open(path_database+file_databbase,'rt')\n",
        "    try:\n",
        "                \n",
        "        reader=csv.reader(outfile)\n",
        "        for row in reader:\n",
        "            queue_paragraph.append(row)\n",
        "            #print (row)\n",
        "    finally:\n",
        "        print (\"row\")\n",
        "        outfile.close()\n",
        "        \n",
        "    return queue_paragraph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yOs-pWWeuaMi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "Write Excell sheet\n",
        "'''\n",
        "def save_file_to_database(data_rows,path_database,file_databbase,header_list):#header_list=['index','text']\n",
        "    outfile = open(path_database+file_databbase,'w')\n",
        "    writer=csv.writer(outfile)\n",
        "    #header_list=['uuid','paragraph','doc_id']\n",
        "    i=0\n",
        "    for line in data_rows:\n",
        "        row=[i,line]#,'paragraph no.'+str(i)]\n",
        "        if i==0:\n",
        "            \n",
        "            writer.writerow(header_list)\n",
        "            writer.writerow(row)\n",
        "        else:\n",
        "            #print('ff')\n",
        "            writer.writerow(row)\n",
        "        i+= 1\n",
        "        #outfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PrjPdQNfwy3K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def pragraph_to_setnences(str):\n",
        "    return sent_tokenize(str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UxHa6tDoGHF4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv   \n",
        "def add_row(row,path_database,file_name):\n",
        "    #fields=['first','second','third']\n",
        "    with open(path_database+file_name, 'a') as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow(row)\n",
        "        print(\"printed\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H00aZ9MQIZoB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## pathes and files"
      ]
    },
    {
      "metadata": {
        "id": "TRIVr4TKuzbS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "path_database='./drive_PH/Colab Notebooks/' \n",
        "path_stop_word=path_database+'input/stopwords/'\n",
        "\n",
        "pragraph_index='pragraph_index_20.csv'\n",
        "Sentences='Sentences_20.csv'\n",
        "Sentences_not_stops='Sentences_not_stops_20.csv'\n",
        "lesk='lesk_20.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e1uCSAvzKHGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Steps"
      ]
    },
    {
      "metadata": {
        "id": "G8DAsqXcJ3mT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Load 20newsgroups dataset"
      ]
    },
    {
      "metadata": {
        "id": "FzyPK8SaloNo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title Load 20newsgroups dataset\n",
        "\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0IcKS15EmGUk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "documents[0]\n",
        "dataset.filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gFmp8dmuP0hs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#save_file_to_database(documents,path_database,pragraph_index,header_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qxfC-Owjmby8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "header_list=['index','text']\n",
        "#save_file_to_database(documents,'/',pragraph_index,header_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VAgZQlckK9wd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##nytimes_news_articles."
      ]
    },
    {
      "metadata": {
        "id": "64fBKXlnBYIa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "def courps_to_CSV_docs():\n",
        "    #Reading the news articles file\n",
        "    nyTimesFile = open('./nytimes_news_articles.txt', encoding='latin-1')\n",
        "    nyTimesFile.seek(0)\n",
        "    nyTimesV1 = nyTimesFile.readlines()\n",
        "    nyTimesTemp = []\n",
        "    nyTimesURL = []\n",
        "\n",
        "    for i in range(0, len(nyTimesV1)-1):\n",
        "        if re.findall('URL', nyTimesV1[i]) == []:\n",
        "            sent = sent + nyTimesV1[i]\n",
        "            if (re.findall('URL', nyTimesV1[i+1]) != []) and (i+1 < len(nyTimesV1)):\n",
        "                nyTimesTemp.append(sent.strip())\n",
        "        else:\n",
        "            sent = ''\n",
        "            nyTimesURL.append(nyTimesV1[i])\n",
        "\n",
        "    for i in range(0, len(nyTimesTemp)):\n",
        "        nyTimesTemp[i] = nyTimesTemp[i]+'articleID'+str(i)\n",
        "    print(len(nyTimesTemp))\n",
        "    header_list=['index','text']\n",
        "    save_file_to_database(nyTimesTemp,path_database,pragraph_index,header_list)\n",
        "    '''for i in range(1):\n",
        "        print(i,\"============================================\")\n",
        "        print(\"============================================\")'''\n",
        "    #nytimes = preProcessor(nyTimesTemp)\n",
        "    print(\"============================================\")\n",
        "    #print(nytimes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "36a6-R2FBcZe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#courps_to_CSV_docs()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_6BmvqLFLazK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Load paragraphs csv file"
      ]
    },
    {
      "metadata": {
        "id": "FXOY0cobvZEW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "paragraphs=read_cvs_by_pands(path_database,pragraph_index,None,0)\n",
        "paragraphs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Iu4dgfNRLfkw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Convert paragraph to sentences"
      ]
    },
    {
      "metadata": {
        "id": "QZ3YiCG0-7O0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def paragraphs_to_sentece(pragraphs):\n",
        "    sentenses_list=[]\n",
        "    \n",
        "    for index_p in  range(len(pragraphs)):\n",
        "        #print(index_p)\n",
        "        #print(\"pppppppppppppppppppp\")\n",
        "        #print(pragraphs[index_p])\n",
        "        if pragraphs[index_p] is not None:\n",
        "          #print(\"setnences\")\n",
        "          #p1=txt_pragraphs(pragraphs[index_p])\n",
        "          setnences=pragraph_to_setnences(str(pragraphs[index_p]))\n",
        "          #print(\"sssssssssssssssssssssssssss\")\n",
        "          #print(\"setnences\")#,setnences)\n",
        "\n",
        "          for indexs in range(len(setnences)):\n",
        "              row=[]\n",
        "              #print(setnences)\n",
        "              row.append(index_p)\n",
        "              row.append(indexs)\n",
        "              row.append(setnences[indexs])\n",
        "              sentenses_list.append(row)\n",
        "    header_list=['index_P','index_sent','sentence']\n",
        "    df = pd.DataFrame(sentenses_list, columns=header_list)#, index=index)\n",
        "    #df\n",
        "\n",
        "    #print(sentenses_list)\n",
        "    df.to_csv(path_database+Sentences, encoding='utf-8', index=False)\n",
        "    #save_file_to_database(sentenses_list,path_database,\"Sentences.csv\",header_list)        \n",
        "    return df\n",
        "\n",
        "#paragraphs_to_sentece(paragraphs.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fTKOLYu3L0Ez",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Load sentences CSV file"
      ]
    },
    {
      "metadata": {
        "id": "WOwVVbU6TXFq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "setences=read_cvs_by_pands(path_database,Sentences,None,0)\n",
        "setences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8NfaNVAwMFMu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Conver sentence to word list\n",
        "remove stop word "
      ]
    },
    {
      "metadata": {
        "id": "MEoC0O_-w7C2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def stopwords_list():\n",
        "    stopwordsFile = open(path_stop_word+'stopwords.txt')\n",
        "    stopwordsFile.seek(0)\n",
        "    stopwordsV1 = stopwordsFile.readlines()\n",
        "    stopwordsV2 = []\n",
        "    for sent in stopwordsV1:\n",
        "        sent.replace('\\n', '')\n",
        "        new_word = sent[0:len(sent) - 1]\n",
        "        stopwordsV2.append(new_word.lower())\n",
        "    return stopwordsV2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYP_EfTsw62c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_stop_words = ['the', 'that', 'to', 'as', 'there', 'has', 'and', 'or', 'is', 'not', 'a', 'of', 'but', 'in', 'by', 'on', 'are', 'it', 'if','what','where','how','when']\n",
        "new_stop_words2=['--','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now','even','until','then','must']\n",
        "numbers=[1,2,3,4,5,6,7,8,9]\n",
        "stopwordsV2=stopwords_list()\n",
        "#stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "def remove_stopword_sentences(str):\n",
        "   \n",
        "            \n",
        "    list_word=[]\n",
        "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
        "    \n",
        "    words=tokenizer.tokenize(str)\n",
        "    for word in words:\n",
        "        new_word = word.encode('ascii', 'ignore').decode('utf-8')\n",
        "        if new_word != '':\n",
        "    \n",
        "            english_stops = set(stopwords.words('english'))\n",
        "           \n",
        "            list_word=[new_word for new_word in words if new_word.lower() not in english_stops\n",
        "                       and new_word.lower() not in new_stop_words \n",
        "                       and new_word.lower() not in new_stop_words2 \n",
        "                       and  not new_word.lower().isdigit() \n",
        "                       and new_word.lower() not in digits \n",
        "                       and new_word.lower() not in  numbers and word.lower() not in stopwordsV2\n",
        "                       and new_word.lower() not in string.punctuation]\n",
        "    \n",
        "  \n",
        "    \n",
        "    return list_word#(stem(setem_word for setem_word in  ([word for word in words if word not in english_stops and word not in new_stop_words])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y3_OiNTTB9Lw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def  sentece_Not_stop_word(setences):\n",
        "    #words_list=[]\n",
        "    sentenses_list=[]\n",
        "    \n",
        "    for index_s in  range(len(setences)):\n",
        "            \n",
        "          #print(\"Sentence No. \",index_s,\": \",setences.loc[index_s]['sentence'],\"\\n\")\n",
        "          words=remove_stopword_sentences(str(setences.loc[index_s]['sentence']))\n",
        "          wordsent=''\n",
        "          row=[]\n",
        "          \n",
        "\n",
        "          row.append(setences.loc[index_s]['index_P'])\n",
        "          row.append(setences.loc[index_s]['index_sent'])\n",
        "          row.append(words)\n",
        "          sentenses_list.append(row)\n",
        "    header_list=['index_P','index_sent','words_not_stop']\n",
        "    df = pd.DataFrame(sentenses_list, columns=header_list)#, index=index)\n",
        "    #df\n",
        "\n",
        "    #print(sentenses_list)\n",
        "    df.to_csv(path_database+Sentences_not_stops, encoding='utf-8', index=False)\n",
        "    #save_file_to_database(sentenses_list,path_database,\"Sentences.csv\",header_list)        \n",
        "    return df\n",
        "\n",
        "#sentece_Not_stop_word(setences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OQuKzaqfMkq5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Load Sentences not stop word"
      ]
    },
    {
      "metadata": {
        "id": "9AecJWi87X8w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df_Sentences_not_stops=read_cvs_by_pands(path_database,Sentences_not_stops,None,0)\n",
        "df_Sentences_not_stops"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lTjZb-JWzFp8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "75LvkDP6CONE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "this function for compute lesk of word in sentence\n",
        "'''\n",
        "def lesk_word_sentence(sentence,words):\n",
        "    from nltk.wsd import lesk\n",
        "    lesk_synset=''\n",
        "    \n",
        "    lesks_row= []\n",
        "    lesks_name=[]\n",
        "    #print(type(words))\n",
        "    for i in range(len(words)):\n",
        "        \n",
        "        disambiguated=lesk(context_sentence=str(sentence), ambiguous_word=words[i].strip())\n",
        "        \n",
        "        \n",
        "        if disambiguated is not None:\n",
        "            lesks_row.append(disambiguated)\n",
        "            lesks_name.append(disambiguated.name())\n",
        "    \n",
        "    \n",
        "    return lesks_row,lesks_name\n",
        "#disambiguated#lesk_synset\n",
        "\n",
        "#lesk(\"Computer science is a discipline that spans theory and practice\",\"science\")\n",
        "\n",
        "#sent = 'people should be able to marry a person of their choice'.split()\n",
        "#ll=['Well','\"im\"','sure','story','nad','seem','biased']\n",
        "#lesk_word_sentence(\"Well i'm not sure about the story nad it did seem biased.\", ll)\n",
        "#lesk('I love dog', 'dog')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LeAahyGoPe1M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def sent_lesks():\n",
        "    df_Sentences=read_cvs_by_pands(path_database,Sentences,None,0)\n",
        "    #print(df_Sentences.columns)\n",
        "    df_Sentences_not_stops=read_cvs_by_pands(path_database,Sentences_not_stops,None,0)\n",
        "    lesks=[]\n",
        "    for i in range(len(df_Sentences_not_stops)):\n",
        "        sent=[]\n",
        "        full_words=df_Sentences_not_stops['words_not_stop'][i].replace(\"[\",'').replace(\"]\",'').replace(\"'\",'').replace('\"','')\n",
        "        full_words_list=full_words.replace(\",\",'-')      \n",
        "        words=full_words.split(',')\n",
        "        sentense=df_Sentences['sentence'][i]\n",
        "        \n",
        "        sent.append(df_Sentences['index_P'][i])\n",
        "        sent.append(df_Sentences['index_sent'][i])\n",
        "        ss=lesk_word_sentence(sentense, words)\n",
        "        \n",
        "        sent.append(full_words_list)\n",
        "        sent.append(ss[0])\n",
        "        sent.append(ss[1])\n",
        "        \n",
        "        lesks.append(sent)\n",
        "        \n",
        "    header_list=['index_P','index_sent','full_word','lesks','lesks_name']\n",
        "    df = pd.DataFrame(lesks, columns=header_list)\n",
        "    \n",
        "    df.to_csv(path_database+'lesk_20.csv', encoding='utf-8', index=False)  \n",
        "    return df\n",
        "    #return lesks\n",
        "#sent_lesks()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IECRFxMiNOCK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##TF-IDF"
      ]
    },
    {
      "metadata": {
        "id": "nocJy3-gN7hz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Creat lesk list per paragraph"
      ]
    },
    {
      "metadata": {
        "id": "gUeT72KCmmLo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def paragarph_to_lesk():\n",
        "    pragraph_list=[]\n",
        "    for p in range(2):#len(index_p_p)) :\n",
        "        #print(p)\n",
        "        l_names=l[l['index_P']==str(p)]['lesks_name']\n",
        "        #print(len(l_names))\n",
        "\n",
        "        l_name_one_paragraph=''\n",
        "        paragraph_row=[]\n",
        "        for i in range(len(l_names)):\n",
        "            l_name_one_paragraph+=l_names.get_values()[i].replace('[','').replace(']',',').strip()\n",
        "\n",
        "        if isNotBlank(l_name_one_paragraph.replace(',','')):\n",
        "\n",
        "\n",
        "            paragraph_row.append(p)\n",
        "            paragraph_row.append(l_name_one_paragraph)\n",
        "            pragraph_list.append(paragraph_row)\n",
        "        else:\n",
        "            print(p,\"pppppppppppppp\")\n",
        "    #print(pragraph_list)\n",
        "    header_list=['index_P','lesk_list']\n",
        "    df = pd.DataFrame(pragraph_list,columns=header_list)\n",
        "    df.to_csv(path_database+'paragraph_lesk_20.csv', encoding='utf-8', index=False)  \n",
        "    return df\n",
        "    #df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-6TXp2QPOLxO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Load paragraph lesk"
      ]
    },
    {
      "metadata": {
        "id": "oQr4yylSohoO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p_lesk=read_cvs_by_pands(path_database,'paragraph_lesk_20.csv',None,0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S1nwoVzzOSET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Calculate TF-IDF"
      ]
    },
    {
      "metadata": {
        "id": "Wxbm9fgrtjLQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tfidf():\n",
        "    cv_tfidf_lesk = TfidfVectorizer(analyzer='word',token_pattern=\"'\"+'(?u)\\\\b\\\\w\\\\w+\\\\b\\\\.\\\\w\\\\.\\\\d\\\\d'+\"'\") #lesk\n",
        "    #print(type(texts_lesk))\n",
        "    cv_tfidf_fit_lesk=cv_tfidf_lesk.fit_transform(p_lesk['lesk_list']).toarray()\n",
        "    df_tfidf_lesk=pd.DataFrame(cv_tfidf_fit_lesk,columns=cv_tfidf_lesk.get_feature_names(),index=p_lesk['index_P'])\n",
        "    df_tfidf_lesk.to_csv(path_database+'tf_idf_lesk_table_20.csv')\n",
        "    #save_file_to_database(texts_lesk,path_database,lesk_paragraph,lesk_paragraph_list)\n",
        "    return df_tfidf_lesk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TLOGFIBPNbKv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Load TF-IDF"
      ]
    },
    {
      "metadata": {
        "id": "FGXU4MQMHF2U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfidf_list=read_cvs_by_pands(path_database,'tf_idf_lesk_table_20.csv',None,0).index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rr1aiMlKOiRf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Word2Vec"
      ]
    },
    {
      "metadata": {
        "id": "oUVEZZoxL7Pm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import gensim.models\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import word2vec\n",
        "import logging\n",
        "from gensim.models import Phrases"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tf_HRAXK7nX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#https://taylorwhitten.github.io/blog/word2vec\n",
        "\n",
        "import gensim\n",
        "import gensim.models\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import word2vec\n",
        "import logging\n",
        "from gensim.models import Phrases\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
        "    level=logging.INFO)\n",
        "\n",
        "num_features = 300    # Word vector dimensionality                      \n",
        "min_word_count = 50   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 6           # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "\n",
        "model = word2vec.Word2Vec(p_lesk['lesk_list'], workers=num_workers, \\\n",
        "            size=num_features, min_count = min_word_count, \\\n",
        "            window = context, sample = downsampling)\n",
        "\n",
        "model.init_sims(replace=True)\n",
        "model_name = \"lesk2vec.bin\"\n",
        "model.wv.save_word2vec_format(path_database+model_name, binary=True)\n",
        "#new_model = gensim.models.Word2Vec.load('lesk2vec')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uht3S2S4RYya",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ll=[['ff','rtert','fgdfgf'],['ff','trt','hgj'],['trt','rtert','tyt']]\n",
        "\n",
        "model2 = gensim.models.KeyedVectors.load_word2vec_format(ll, binary=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xqQ1YiDOAg3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#model.most_similar('yea.r.01',  topn=15)\n",
        "new_model =  gensim.models.KeyedVectors.load_word2vec_format(path_database+'lesk2vec.bin', binary=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_5n3yvkvElrC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_model.wv.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RIfVEgdaCcsC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zRzbslo-Cv0M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l=read_cvs_by_pands(path_database,'lesk_20.csv',None,0)\n",
        "l"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r-viOg2QGplO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "p=read_cvs_by_pands(path_database,'pragraph_index_20.csv',None,0)\n",
        "#index_p_p=p['index']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vNiJw3RnG2tA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "l_names=[]\n",
        "for i in range(len(index_p_p)) :\n",
        "    l_names=l[l['index_P']==str(i)]['lesks_name']\n",
        "print(l_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iCxyYidh13d-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "len(index_p_p)\n",
        "l_names=l[l['index_P']==str(11313)]['lesks_name']\n",
        "l_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "joKLoGX-rgd4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cq22vLjIxEh6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(p_lesk['lesk_list'][1:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mUN-6zv7PSW8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ku2JxpIMYGxo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print (len(all_feature))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G_oIWNfJPL-Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Semantic similarty Lesk"
      ]
    },
    {
      "metadata": {
        "id": "M_WXdPGcRbc6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d08e54e6-fa90-474d-8011-a1b3c9f34bd5"
      },
      "cell_type": "code",
      "source": [
        "def isNotBlank (myString):\n",
        "    if myString and myString.strip():\n",
        "        #myString is not None AND myString is not empty or blank\n",
        "        return True\n",
        "    #myString is None OR myString is empty or blank\n",
        "    return False\n",
        "isNotBlank(' ')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "grqQT2aQWpiw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def similarity_by_infocontent(sense1, sense2, option):\n",
        "    #sense1=\"Synset('\"+sense1+\"')\"\n",
        "    #sense2=\"Synset('\"+sense2+\"')\"\n",
        "    sense1 = wn.synset(sense1)\n",
        "    sense2 = wn.synset(sense2)\n",
        "    #print(sense1,sense2)\n",
        "    \"\"\" Returns similarity scores by information content. \"\"\"\n",
        "    #if sense1.pos != sense2.pos: # infocontent sim can't do diff POS.\n",
        "        #return 0\n",
        "\n",
        "    info_contents = ['ic-bnc-add1.dat', 'ic-bnc-resnik-add1.dat', \n",
        "                     'ic-bnc-resnik.dat', 'ic-bnc.dat', \n",
        "\n",
        "                     'ic-brown-add1.dat', 'ic-brown-resnik-add1.dat', \n",
        "                     'ic-brown-resnik.dat', 'ic-brown.dat', \n",
        "\n",
        "                     'ic-semcor-add1.dat', 'ic-semcor.dat',\n",
        "\n",
        "                     'ic-semcorraw-add1.dat', 'ic-semcorraw-resnik-add1.dat', \n",
        "                     'ic-semcorraw-resnik.dat', 'ic-semcorraw.dat', \n",
        "\n",
        "                     'ic-shaks-add1.dat', 'ic-shaks-resnik.dat', \n",
        "                     'ic-shaks-resnink-add1.dat', 'ic-shaks.dat', \n",
        "\n",
        "                     'ic-treebank-add1.dat', 'ic-treebank-resnik-add1.dat', \n",
        "                     'ic-treebank-resnik.dat', 'ic-treebank.dat']\n",
        "\n",
        "    if option in ['res', 'resnik']:\n",
        "        #return wn.res_similarity(sense1, sense2, wnic.ic('ic-bnc-resnik-add1.dat'))\n",
        "        #print('simRe snik (c1,c2) = -log p(lso(c1,c2)) = IC(lso(c1,c2)')\n",
        "        return wn.res_similarity(sense1, sense2, wnic.ic('ic-treebank-resnik-add1.dat'))\n",
        "    #return min(wn.res_similarity(sense1, sense2, wnic.ic(ic)) \\\n",
        "    #             for ic in info_contents)\n",
        "\n",
        "    elif option in ['jcn', \"jiang-conrath\"]:\n",
        "        #return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
        "        #print('sim(jcn) (c1,c2 )= (IC(c1) + IC(c2 )) - 2IC(lso(c1,c2 ))')\n",
        "        return wn.jcn_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
        "\n",
        "    elif option in ['lin']:\n",
        "        #return wn.lin_similarity(sense1, sense2, wnic.ic('ic-bnc-add1.dat'))\n",
        "        #print('sim(lin) (c1,c2)=(2IC(lso(c1,c2 )))/(IC(c1)+IC(c2))')\n",
        "        return wn.lin_similarity(sense1, sense2, wnic.ic('ic-treebank.dat'))\n",
        "\n",
        "def sim(sense1, sense2, option=\"path\"):\n",
        "    \"\"\" Calculates similarity based on user's choice. \"\"\"\n",
        "    option = option.lower()\n",
        "    if option.lower() in [\"path\", \"path_similarity\", \n",
        "                        \"wup\", \"wupa\", \"wu-palmer\", \"wu-palmer\",\n",
        "                        'lch', \"leacock-chordorow\"]:\n",
        "        return similarity_by_path(sense1, sense2, option) \n",
        "    elif option.lower() in [\"res\", \"resnik\",\n",
        "                          \"jcn\",\"jiang-conrath\",\n",
        "                          \"lin\"]:\n",
        "        return similarity_by_infocontent(sense1, sense2, option)\n",
        "\n",
        "def max_similarity(context_sentence, ambiguous_word, option=\"path\", \n",
        "                   pos=None, best=True):\n",
        "    \"\"\"\n",
        "    Perform WSD by maximizing the sum of maximum similarity between possible \n",
        "    synsets of all words in the context sentence and the possible synsets of the \n",
        "    ambiguous words (see http://goo.gl/XMq2BI):\n",
        "    {argmax}_{synset(a)}(\\sum_{i}^{n}{{max}_{synset(i)}(sim(i,a))}\n",
        "    \"\"\"\n",
        "    result = {}\n",
        "    for i in wn.synsets(ambiguous_word):\n",
        "        try:\n",
        "            if pos and pos != str(i.pos()):\n",
        "                continue\n",
        "        except:\n",
        "            if pos and pos != str(i.pos):\n",
        "                continue\n",
        "        result[i] = sum(max([sim(i,k,option) for k in wn.synsets(j)]+[0]) \\\n",
        "                        for j in word_tokenize(context_sentence))\n",
        "\n",
        "    if option in [\"res\",\"resnik\"]: # lower score = more similar\n",
        "        result = sorted([(v,k) for k,v in result.items()])\n",
        "    else: # higher score = more similar\n",
        "        result = sorted([(v,k) for k,v in result.items()],reverse=True)\n",
        "    #print (result)\n",
        "    if best: return result[0][1];\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J3C3ilxSVTBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "24f44072-c3eb-4993-a7c6-6d4be962ecc9"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "calculate simantic simelart for Dimensionality reduction vector\n",
        "say vector is n element [n1,n2,n3,.....nm], data frame row=n,col=n\n",
        "sim(n[row],n[col])if if row != col:\n",
        "option is sim method like res,lin,jcn ...... for IC\n",
        "\n",
        "'''\n",
        "#print(type(tfidf_feature_names))\n",
        "#tfidf_feature_names\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet_ic')\n",
        "def sim_docs_lesk(df_freq,option):\n",
        "    \n",
        "    \n",
        "    series=list(df_freq)#pd.Series(data=lesk_vec)\n",
        "    #series.drop_duplicates()\n",
        "    synset_lesk_noDuplicates= series#.tolist()    \n",
        "    #df_all_synset_lesk = pd.DataFrame(index=series, columns=series )\n",
        "    #df_all_synset_lesk = pd.DataFrame(columns=series ) **********\n",
        "    #print (\"synset_lesk_noDuplicates\",len(synset_lesk_noDuplicates))\n",
        "    \n",
        "    for row in range(len(synset_lesk_noDuplicates)):\n",
        "        #try:\n",
        "            row_csv=[]\n",
        "            row_csv.append(synset_lesk_noDuplicates[row])\n",
        "            print(\"1\",row_csv)\n",
        "            #data_row=[]**********\n",
        "            for col in range(len(synset_lesk_noDuplicates)):\n",
        "\n",
        "                #if row < col:\n",
        "                try:\n",
        "                    sim=similarity_by_infocontent(series[row],series[col],option)\n",
        "                    if sim is not None:\n",
        "                        #data_row.append(sim)***********\n",
        "                        row_csv.append(sim)\n",
        "                    else:\n",
        "                        #data_row.append(0)*************\n",
        "                        #print(len(data_row))\n",
        "                        row_csv.append(0)\n",
        "                except  Exception as inst:\n",
        "                    #data_row.append(0)*********\n",
        "                    row_csv.append(0)\n",
        "                    #print(type(inst))    # the exception instance\n",
        "                    #print(inst.args)     # arguments stored in .args\n",
        "                    #print(inst)          # __str__ allows args to be printed directly,\n",
        "                    pass\n",
        "                    #print(\"Ex\")\n",
        "\n",
        "            #print(\"2\",row_csv)\n",
        "            add_row(row_csv,path_database,\"res_sem_sim_table.csv\")\n",
        "            #df_all_synset_lesk.loc[series[row]]=data_row**********\n",
        "       \n",
        "    #return df_all_synset_lesk\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /content/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /content/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "v6B1qg9LWuvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bbca9555-6d87-42c3-c528-a55b07d7ef21"
      },
      "cell_type": "code",
      "source": [
        "f=open(path_database+'tf_idf_lesk_table_20.csv')\n",
        "all_feature=f.readline().replace(\"'\",'').split(',')[1:]\n",
        "headers=all_feature.copy()\n",
        "\n",
        "headers.insert( 0, \" \")\n",
        "#add_row(headers,path_database,\"res_sem_sim_table.csv\")\n",
        "#print (all_feature)\n",
        "#print (headers)\n",
        "#print (len(all_feature))\n",
        "#df_sim_sem_lesk=\n",
        "sim_docs_lesk(all_feature,'res')\n",
        "#df_sim_sem_lesk.to_csv(path_database+'res_sem_sim_table.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 ['1760s.n.01']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LtYbuW6KXJYw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "37d1db13-d75e-4b23-fdd6-1966bb048b87"
      },
      "cell_type": "code",
      "source": [
        "res_sem_sim_table=read_cvs_by_pands(path_database,'res_sem_sim_table.csv',None,0)\n",
        "res_sem_sim_table"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>1760s.n.01</th>\n",
              "      <th>1830s.n.01</th>\n",
              "      <th>1840s.n.01</th>\n",
              "      <th>1850s.n.01</th>\n",
              "      <th>1870s.n.01</th>\n",
              "      <th>1900s.n.01</th>\n",
              "      <th>aachen.n.01</th>\n",
              "      <th>aardvark.n.01</th>\n",
              "      <th>aaron.n.02</th>\n",
              "      <th>...</th>\n",
              "      <th>zoological.a.02</th>\n",
              "      <th>zoologist.n.01</th>\n",
              "      <th>zoom.v.02</th>\n",
              "      <th>zoroaster.n.01</th>\n",
              "      <th>zoroastrian.n.01</th>\n",
              "      <th>zoroastrianism.n.01</th>\n",
              "      <th>zu.n.01</th>\n",
              "      <th>zurich.n.01</th>\n",
              "      <th>zurvanism.n.02</th>\n",
              "      <th>zymosis.n.01</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1760s.n.01</td>\n",
              "      <td>13.523224</td>\n",
              "      <td>7.652986</td>\n",
              "      <td>7.652986</td>\n",
              "      <td>7.652986</td>\n",
              "      <td>7.652986</td>\n",
              "      <td>7.652986</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.622355</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.622355</td>\n",
              "      <td>0.622355</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.622355</td>\n",
              "      <td>-0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 20118 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               1760s.n.01  1830s.n.01  1840s.n.01  1850s.n.01  1870s.n.01  \\\n",
              "0  1760s.n.01   13.523224    7.652986    7.652986    7.652986    7.652986   \n",
              "\n",
              "   1900s.n.01  aachen.n.01  aardvark.n.01  aaron.n.02       ...        \\\n",
              "0    7.652986         -0.0           -0.0    0.622355       ...         \n",
              "\n",
              "   zoological.a.02  zoologist.n.01  zoom.v.02  zoroaster.n.01  \\\n",
              "0                0            -0.0          0            -0.0   \n",
              "\n",
              "   zoroastrian.n.01  zoroastrianism.n.01   zu.n.01  zurich.n.01  \\\n",
              "0              -0.0             0.622355  0.622355         -0.0   \n",
              "\n",
              "   zurvanism.n.02  zymosis.n.01\\n  \n",
              "0        0.622355            -0.0  \n",
              "\n",
              "[1 rows x 20118 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "1dOgLaYhlp0S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PtJWZ31uGjHM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}